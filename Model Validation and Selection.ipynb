{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Validation and Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will be going through varies methods to select a model based upon the following features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "\t<li>cross validation techniques</li>\n",
    "\t<li>Bias -variance tradeoff</li>\n",
    "\t<li>regularization (L1, L2, Elastic net)</li>\n",
    "\t<li>grid search options.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cross validation techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation is a technique for evaluating ML models by training several ML models on subsets of the available input data and evaluating them on the complementary subset of the data. Use cross-validation to detect overfitting, ie, failing to generalize a pattern.\n",
    "\n",
    "In ML, you can use the k-fold cross-validation method to perform cross-validation. In k-fold cross-validation, you split the input data into k subsets of data (also known as folds). You train an ML model on all but one (k-1) of the subsets, and then evaluate the model on the subset that was not used for training. This process is repeated k times, with a different subset reserved for evaluation (and excluded from training) each time.\n",
    "\n",
    "<img src=\"./images/cross_validation.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.read_csv(\"./Data/new_data_feature.csv\")\n",
    "data = pd.get_dummies(data, columns=[\"timeofDay\",\"activeStatus\",'dayoftheweek','WeekDayType','season'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select response y and Training set X\n",
    "k = list(data)\n",
    "k\n",
    "my_cols = [\n",
    " 'lights',\n",
    " 'T1',\n",
    " 'RH_1',\n",
    " 'T2',\n",
    " 'RH_2',\n",
    " 'T3',\n",
    " 'RH_3',\n",
    " 'T4',\n",
    " 'RH_4',\n",
    " 'T5',\n",
    " 'RH_5',\n",
    " 'T6',\n",
    " 'RH_6',\n",
    " 'T7',\n",
    " 'RH_7',\n",
    " 'T8',\n",
    " 'RH_8',\n",
    " 'T9',\n",
    " 'RH_9',\n",
    " 'T_out',\n",
    " 'Press_mm_hg',\n",
    " 'RH_out',\n",
    " 'Windspeed',\n",
    " 'Visibility',\n",
    " 'Tdewpoint',\n",
    " 'NSM',\n",
    " 'weekOfTheYear',\n",
    " 'timeofDay_Day',\n",
    " 'timeofDay_Night',\n",
    " 'activeStatus_awake',\n",
    " 'activeStatus_sleep',\n",
    " 'dayoftheweek_Friday',\n",
    " 'dayoftheweek_Monday',\n",
    " 'dayoftheweek_Saturday',\n",
    " 'dayoftheweek_Sunday',\n",
    " 'dayoftheweek_Thurday',\n",
    " 'dayoftheweek_Tuesday',\n",
    " 'dayoftheweek_Wednesday',\n",
    " 'WeekDayType_Weekday',\n",
    " 'WeekDayType_weekend',\n",
    " 'season_spring',\n",
    " 'season_winter']\n",
    "X = data[my_cols].values\n",
    "y = data['Appliances']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[my_cols]\n",
    "y = data['Appliances']\n",
    "# min_max_scaler = MinMaxScaler()\n",
    "# X = min_max_scaler.fit_transform(X)\n",
    "# y = min_max_scaler.fit_transform(y.reshape(1, -1))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,train_size=0.75, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "scores = cross_val_score(rf, X, y, cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias -variance tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Overview of Bias and Variance In supervised machine learning an algorithm learns a model from training data.</p>\n",
    "\n",
    "<p>The goal of any supervised machine learning algorithm is to best estimate the mapping function (f) for the output variable (Y) given the input data (X). The mapping function is often called the target function because it is the function that a given supervised machine learning algorithm aims to approximate.</p>\n",
    "<p>The prediction error for any machine learning algorithm can be broken down into three parts:</p>\n",
    "<p>&nbsp;</p>\n",
    "<ol>\n",
    "\t<li>Bias Error</li>\n",
    "\t<li>Variance Error</li>\n",
    "\t<li>Irreducible Error</li>\n",
    "</ol>\n",
    "<p>&nbsp;</p>\n",
    "<p>The irreducible error cannot be reduced regardless of what algorithm is used. It is the error introduced from the chosen framing of the problem and may be caused by factors like unknown variables that influence the mapping of the input variables to the output variable.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Bias Error</h4>\n",
    "\n",
    "<p>Bias are the simplifying assumptions made by a model to make the target function easier to learn.</p>\n",
    "\n",
    "<p>Generally, parametric algorithms have a high bias making them fast to learn and easier to understand but generally less flexible. In turn, they&nbsp;have lower predictive performance on complex problems that fail to meet the simplifying assumptions of the algorithms bias.</p>\n",
    "\n",
    "<ul>\n",
    "\t<li><strong>Low Bias</strong>: Suggests less assumptions about the form of the target function.</li>\n",
    "\t<li><strong>High-Bias</strong>:&nbsp;Suggests more assumptions about the form of the target function.</li>\n",
    "</ul>\n",
    "\n",
    "<p>Examples of low-bias machine learning algorithms&nbsp;include:&nbsp;Decision Trees,&nbsp;k-Nearest Neighbors and&nbsp;Support Vector Machines.</p>\n",
    "\n",
    "<p>Examples of high-bias machine learning algorithms include:&nbsp;Linear Regression,&nbsp;Linear Discriminant Analysis and&nbsp;Logistic Regression.</p>\n",
    "\n",
    "<h4>Variance Error</h4>\n",
    "\n",
    "<p>Variance is the amount that the estimate of the target function will change if different training data was used.</p>\n",
    "\n",
    "<p>The target function is estimated from the training data by a machine learning algorithm, so we should expect the algorithm to have some variance. Ideally, it should not change too much from one training dataset to the next, meaning that the algorithm is good at picking out the hidden underlying mapping between the inputs and the output variables.</p>\n",
    "\n",
    "<p>Machine learning algorithms that have a high variance are strongly influenced by the specifics of the training data. This means that the specifics of the training have&nbsp;influences the number and types of parameters used to characterize the mapping function.</p>\n",
    "\n",
    "<ul>\n",
    "\t<li><strong>Low Variance</strong>: Suggests small changes to the estimate of the target function with changes to the training dataset.</li>\n",
    "\t<li><strong>High Variance</strong>: Suggests large changes to the estimate of the target function with changes to the training dataset.</li>\n",
    "</ul>\n",
    "\n",
    "<p>Generally, nonparametric machine learning algorithms that have a lot of flexibility have a high variance. For example, decision trees have a high variance, that is even higher if the trees are not pruned before use.</p>\n",
    "\n",
    "<p>Examples of low-variance machine learning algorithms include:&nbsp;Linear Regression,&nbsp;Linear Discriminant Analysis and&nbsp;Logistic Regression.</p>\n",
    "\n",
    "<p>Examples of high-variance machine learning algorithms include:&nbsp;Decision Trees,&nbsp;k-Nearest Neighbors and&nbsp;Support Vector Machines.</p>\n",
    "\n",
    "<h4>Bias-Variance Trade-Off</h4>\n",
    "\n",
    "<p>The goal of any supervised machine learning algorithm is to achieve low bias and low variance. In turn the algorithm should achieve good prediction performance.</p>\n",
    "\n",
    "<p>You can see a general trend in the examples above:</p>\n",
    "\n",
    "<ul>\n",
    "\t<li>Parametric or linear machine learning algorithms often have a high bias but a low variance.</li>\n",
    "\t<li>Non-parametric or non-linear machine learning algorithms often have a low bias but a high variance.</li>\n",
    "</ul>\n",
    "\n",
    "<p>The parameterization of machine learning algorithms is often a battle to balance out bias and variance.</p>\n",
    "\n",
    "<p>Below are two examples of configuring the bias-variance trade-off for specific algorithms:</p>\n",
    "\n",
    "<ul>\n",
    "\t<li>The k-nearest neighbors algorithm has low bias and high variance, but the trade-off can be changed by increasing the value of k which increases the number of neighbors that contribute t the prediction and in turn increases the bias of the model.</li>\n",
    "\t<li>The support vector machine algorithm has low bias and high variance, but the trade-off can be changed by increasing the C parameter that influences the number of violations of the margin allowed in the training data which increases the bias but decreases the variance.</li>\n",
    "</ul>\n",
    "\n",
    "<p>There is no escaping&nbsp;the relationship between bias and variance in machine learning.</p>\n",
    "\n",
    "<ul>\n",
    "\t<li>Increasing the bias will decrease the variance.</li>\n",
    "\t<li>Increasing the variance will decrease the bias.</li>\n",
    "</ul>\n",
    "\n",
    "<p>There is a trade-off at play between these two concerns and the algorithms you choose and the way you choose to configure them are finding different balances in this trade-off for your problem</p>\n",
    "\n",
    "<p>In reality, we cannot calculate the real bias and variance error terms because we do not know the actual underlying target function. Nevertheless, as a framework, bias and variance provide the tools to understand the behavior of machine learning algorithms in the pursuit of predictive performance.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization (L1, L2, Elastic net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization is used to fix over fitting of models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"./images/eqn2248.png\" /></p>\n",
    "\n",
    "<p>Regularization adds a penalty on the different parameters of the model to reduce the freedom of the model. Hence, the model will be<strong>&nbsp;less likely to fit the noise</strong>&nbsp;of the training data and will improve the generalization abilities of the model. We will study and compare:</p>\n",
    "\n",
    "<ul>\n",
    "\t<li>The L1 regularization (also called Lasso)</li>\n",
    "\t<li>The L2 regularization (also called Ridge)</li>\n",
    "\t<li>The L1/L2 regularization (also called Elastic net)</li>\n",
    "</ul>\n",
    "\n",
    "<p>You can find the R code for regularization at the end of the post.</p>\n",
    "\n",
    "<p>L1 Regularization (Lasso penalisation)</p>\n",
    "\n",
    "<p>The L1 regularization adds a penalty equal to the sum of the absolute value of the coefficients.&nbsp;The L1 regularization will&nbsp;<strong>shrink some parameters to zero</strong>. Hence some variables will not play any role in the model, L1 regression can be seen as a way to select&nbsp;features in a model.&nbsp;</p>\n",
    "\n",
    "<p>&nbsp;As lambda grows bigger, more coefficient will be cut. Below is the evolution of the value of the different coefficients while lambda is growing.</p>\n",
    "\n",
    "<p><img alt=\"\" src=\"./images/L1regularization.png\" /></p>\n",
    "\n",
    "<p>As expected, coefficients are cut one by one until no variables remain. Let&rsquo;s see how the test error is evolving:</p>\n",
    "\n",
    "<p><img alt=\"\" src=\"./images/TestErrorL1Regularization.png\" /></p>\n",
    "\n",
    "<p>At the beginning, cutting coefficient reduces the overfitting and the generalization abilities of the model. Hence, the test error is decreasing. However, as we are cutting more and more coefficient, the test error start increasing. The model is not able to learn complex pattern with so few variables.</p>\n",
    "\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "<p>&nbsp;</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>L2 Regularization (Ridge penalisation)</h4>\n",
    "\n",
    "<p>The L2 regularization adds a penalty equal to the sum of the squared value of the coefficients.</p>\n",
    "\n",
    "<p><img alt=\"\" src=\"./images/CodeCogsEqn-1.gif\" /></p>\n",
    "\n",
    "<p>The L2 regularization will force&nbsp;<strong>the parameters to be relatively small</strong>, the bigger the penalization, the smaller (and the more robust) the coefficients are.</p>\n",
    "\n",
    "<p><img alt=\"\" src=\"./images/L2regularization.png\" /></p>\n",
    "\n",
    "<p>When we compare this plot to the L1 regularization plot, we notice that the coefficients decrease progressively and are not cut to zero. They slowly decrease to zero. That is the behavior we expected. Let&rsquo;s see how the test error evolves:</p>\n",
    "\n",
    "<p><img alt=\"\" src=\"./images/L2RegularizationError.png\" /></p>\n",
    "\n",
    "<p>&nbsp;</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Elastic-net</h4>\n",
    "\n",
    "<p>Elastic-net is a mix of&nbsp;<strong>both L1 and L2 regularizations</strong>. A penalty is applied to the sum of the absolute values and to the sum of the squared values:</p>\n",
    "\n",
    "<p><img alt=\"\" src=\"./images/CodeCogsEqn-3.gif\" /></p>\n",
    "\n",
    "<p>Lambda is a shared penalization parameter while alpha sets the ratio between L1 and L2 regularization in the Elastic Net Regularization. Hence, we expect a hybrid&nbsp;behavior between L1 and L2 regularization.</p>\n",
    "\n",
    "<p><img alt=\"\" src=\"./images/ElasticNetRegularization.png\" /></p>\n",
    "\n",
    "<p>And that&rsquo;s happening: Though coefficients are cut, the cut is less abrupt than the cut with lasso penalization alone.</p>\n",
    "\n",
    "<p><img alt=\"\" src=\"./images/Elastic-Net-Error.png\" /></p>\n",
    "\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "<h4>A geometric perspective on regularization</h4>\n",
    "\n",
    "<p>The Lasso, Ridge and Elastic-net regression can also be viewed as a constraint added to the optimization process.<br />\n",
    "The Lasso&nbsp;error minimization can be rewritten:</p>\n",
    "\n",
    "<p><img alt=\"\" src=\"./images/ErrorConstraintL1.gif\" /></p>\n",
    "\n",
    "<p>And the ridge error minimization can be rewritten:</p>\n",
    "\n",
    "<p><img alt=\"\" src=\"./images/ErrorConstraintL2.gif\" /></p>\n",
    "\n",
    "<p>When written in this way, it&rsquo;s clear that Lasso restricts the coefficients to a square shape (or an L1 sphere) which diagonals are equal to 2t. The ridge error restricts the coefficients to a circle (or an L2 sphere) of radius t.&nbsp;</p>\n",
    "\n",
    "<p><img alt=\"\" src=\"./images/sphx_glr_plot_sgd_penalties_001.png\" /></p>\n",
    "\n",
    "<p>&nbsp;</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search Options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search means you have a set of models (which differ from each other in their parameter values, which lie on a grid). What you do is you then train each of the models and evaluate it using cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>A search consists of:</p>\n",
    "\n",
    "<ul>\n",
    "\t<li>an estimator (regressor or classifier such as&nbsp;<code>sklearn.svm.SVC()</code>);</li>\n",
    "\t<li>a parameter space;</li>\n",
    "\t<li>a method for searching or sampling candidates;</li>\n",
    "\t<li>a cross-validation scheme; and</li>\n",
    "\t<li>a&nbsp;<a href=\"http://scikit-learn.org/stable/modules/grid_search.html#gridsearch-scoring\">score function</a>.</li>\n",
    "</ul>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
